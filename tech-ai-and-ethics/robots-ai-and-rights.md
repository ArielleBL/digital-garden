---
description: Notes from a discussion session I led, summer 2022
---

# Robots, AI, & Rights

This session covered two main areas:&#x20;

* Can AI (and therefore robots) ever be sentient? What would this mean?&#x20;
* The problems with AI & your rights

_Sadly I didn't capture much of the discussion because the audience also had a lot of interesting points around these points and views varied from "generalised AI is just a step away" to mediations on the very nature of life._&#x20;

**Robot Sentience**

The concept of inanimate objects being sentient is not a new one: humans have been imbuing rocks, lightning, large bodies of water & tectonic activity with intentions, emotions & deliberate actions for millennia through myths, legends and religion.

What makes an entity conscious, sentient, self-aware? How can we as outsiders tell? For example, animals appear to engage in human-like behaviour: elephants grieve, dogs weep tears of joy when their owners return, crows will hold a grudge against others who harm their offspring. But you can also break down some seemingly complex behaviours in animals into simple, trainable steps, with basic physiological and environmental inputs, no consciousness needed.

**Anthropomorphisation** (where we ascribe human characteristics and feelings to animals) means we can sometimes also infer a behaviour has a different intent behind it than what is. There's been a huge amount of long-running debate in the animal behaviour world as to what mental abilities and functions humans demonstrate that animals lack such as self-awareness - the difficulty for the side which insists various animals do demonstrate these features is that you can't simply ask animals why they're performing a behaviour. Their internal minds are unknown. So you can't say for certain if they remember a particular time or hold a grudge because of a specific event or because another neurological process is in play. However, we can test if a cat or a monkey for example is self aware using something called the mirror self recognition test - the animal is knocked out and a mark placed on their body somewhere they wouldn't normally see, then they're given a mirror - their response is used to indicate whether they recognise the mark is on their body as opposed to that of a different animal.

Octopi, though they can mount sophisticated escape attempts from aquaria, have essentially individual brains in each limb and appear to play with objects, do not pass this test - same with cats and dogs who we normally assign anthropomorphised emotions and awareness to. This digression into animals is an attempt to explain how difficult it is with existing psychological and neurological tests to even confidently assign mental capacity to other entities, even with decades of research, due to our own biases and the limitations of interspecies communication.

When we now think about assessing the sentience of robots powered by artificial intelligence, we see that some of the same issues are present but they're even harder to spot because robots can also talk to us in our own language - which can make anthropomorphising them even easier. The traditional bar for this is called the **Turing** test - which is when a computer convinces a human that they are speaking with a human being on the other side of a text based interaction. Criticism of this from the **Chinese room perspective** - John Searle. It's not clear even if the computer convinces a native Chinese speaker that they are Chinesd, that they actually understand the words that the program is producing as output.

For example, an AI which has been trained on English language media, such as books, films or plays could be familiar with computers that assert their humanity through a fear of being turned off: Hal in 2001 a space Odyssey, the replicants in Blade Runner, or numerous examples in pop culture recently (Humans the TV show for example) - ask GPT-3 to beg for its life and it will generate a passable script claiming not to be evil and begging for mercy. The reason I used this example - researcher at Google became convinced the chat bot he was working on had become sentient and was expressing fear about being turned off.

So for now, its still pretty clear cut that AI & therefore robots are not sentient - but that also presents problems given the increasing use of this technology in society - if AI isn't sentient it can't have a conscience, develop morals, or think creatively. But we may come to a point where AI is sufficiently generalised that it can be considered indistinguishable from a human.

So questions for the audience:&#x20;

* What would convince you that an AI was conscious or sentient and why?
* How do you think society as a whole should approach continued development towards generalised & conscious AI?
